{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfa26215-c9aa-43bf-98ba-45612aa1aff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1262e4f1-a9e8-47d4-97cc-19d5b7077172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (1024) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "import llama_cpp\n",
    "\n",
    "llm = llama_cpp.Llama(\n",
    "    model_path=\"/home/ubuntu/ai-engineering/models/qwen2.5-7b-instruct-q5_k_m-00001-of-00002.gguf\",\n",
    "    n_gpu_layers=-1,        # to use gpu\n",
    "    n_ctx=1024,           \n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad2225bf-8099-4df9-aef6-7e8b66c57f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion(prompt: str) -> str:\n",
    " system_prompt = \"Answer in two concise sentences.\"\n",
    " generation_params = {\n",
    "  \"temperature\": 0.7,\n",
    "  \"top_p\": 0.9,\n",
    "  \"stop\": [\"<|im_end|>\", \"<|endoftext|>\"],\n",
    "  \"max_tokens\": 2048,\n",
    " }\n",
    " \n",
    " answer = llm.create_chat_completion(\n",
    "  messages=[\n",
    "   {\"role\": \"system\", \"content\": system_prompt},\n",
    "   {\"role\": \"user\", \"content\": prompt}\n",
    "  ],\n",
    "  **generation_params\n",
    " )\n",
    " \n",
    " return answer[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d632c939-1a5a-4f21-a3ef-aef254447437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coffee is a brewed beverage prepared from roasted coffee beans, which are the seeds of berries from the Coffea plant.\n",
      "It depends on the context; generally, a balanced diet and regular exercise are good for health. Specific health impacts vary based on individual circumstances and habits.\n"
     ]
    }
   ],
   "source": [
    "print(chat_completion(prompt=\"What is coffee?\"))\n",
    "print(chat_completion(prompt=\"Is it good for your health?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5482de36-9a0a-4938-abac-2afcb7afe8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_WINDOW        = 4096   # Model context length in tokens\n",
    "MAX_GENERATION_TOKENS = 1024   # Reserved for the model's reply\n",
    "SAFETY_MARGIN         = 32     # Stop tokens, unknowns, wiggle room\n",
    "K_TURNS               = 10     # Max two way conversations kept in memory\n",
    "SYSTEM_PROMPT         = \"Answer in two concise sentences.\"\n",
    "\n",
    "PROMPT_BUDGET = CONTEXT_WINDOW - MAX_GENERATION_TOKENS - SAFETY_MARGIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89e0aa73-78e9-43eb-bb6e-9a11c9da5930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "import llama_cpp\n",
    "from collections import deque \n",
    "\n",
    "llm = llama_cpp.Llama(\n",
    "    model_path=\"/home/ubuntu/ai-engineering/models/qwen2.5-7b-instruct-q5_k_m-00001-of-00002.gguf\",\n",
    "    n_ctx=CONTEXT_WINDOW,\n",
    "    n_gpu_layers=-1,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18941530-179d-4f71-b903-196e40e9fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_tokens(text: str) -> int:\n",
    " \"\"\"Return the exact token count as the model sees it.\"\"\"\n",
    " return len(llm.tokenize(text.encode(), special=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a346861-ef58-407e-95bc-c258333bd068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_msg(role: str, content: str) -> dict:\n",
    " return {\n",
    "  \"role\": role,\n",
    "  \"content\": content,\n",
    "  \"n_tokens\": n_tokens(content),\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5553feb3-d6b7-4e4f-878d-07c4c35b46e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([], maxlen=20)\n"
     ]
    }
   ],
   "source": [
    "history = deque(maxlen=K_TURNS * 2)\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca9a05da-3d82-4d3c-869c-fb6884664c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_window(current_user_msg: dict) -> list[dict]:\n",
    " \"\"\"\n",
    " Assemble [system] + slice(history) + [current user] <= PROMPT_BUDGET.\n",
    " Returns the list ready for the model.\n",
    " \"\"\"\n",
    " used = n_tokens(SYSTEM_PROMPT) + current_user_msg[\"n_tokens\"]\n",
    " window = [make_msg(\"system\", SYSTEM_PROMPT)]\n",
    " \n",
    " # Newest to oldest it will skip latest user turn\n",
    " for msg in reversed(list(history)[:-1]):\n",
    "  if used + msg[\"n_tokens\"] <= PROMPT_BUDGET:\n",
    "   window.insert(1, msg)   \n",
    "   used += msg[\"n_tokens\"]\n",
    "  else:\n",
    "   break                    # Window is full\n",
    " \n",
    " window.append(current_user_msg)\n",
    " return window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88888fd9-8d98-4e63-9390-790a33e4ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_PARAMS = dict(\n",
    " max_tokens=MAX_GENERATION_TOKENS,\n",
    " temperature=0.7,\n",
    " top_p=0.9,\n",
    " stop=[\"<|im_end|>\", \"<|endoftext|>\"],\n",
    ")\n",
    "\n",
    "def answer(user_text: str) -> str:\n",
    " # 1. Log incoming turn\n",
    " user_msg = make_msg(\"user\", user_text)\n",
    " history.append(user_msg)\n",
    " \n",
    " # 2. Craft prompt window\n",
    " prompt_window = build_window(user_msg)\n",
    " \n",
    " # 3. Ask the model\n",
    " reply = llm.create_chat_completion(\n",
    "  messages=prompt_window,\n",
    "  **GEN_PARAMS,\n",
    " )[\"choices\"][0][\"message\"][\"content\"]\n",
    " \n",
    " # 4. Store assistant turn\n",
    " history.append(make_msg(\"assistant\", reply))\n",
    " return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06e66b00-0eda-4d9a-ae7a-ba39ee367f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You study software engineering, and you are at FAST-NUCES.\n",
      "No of Tokens: 15\n"
     ]
    }
   ],
   "source": [
    "query = \"My name is Ali, I study software engineering at FAST-NUCES.\"\n",
    "print(answer(query))\n",
    "print(f\"No of Tokens: {n_tokens(query)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b934b29f-e240-4347-857c-21cc4b9bab7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You study software engineering, and you are at FAST-NUCES.\n",
      "No of Tokens: 9\n"
     ]
    }
   ],
   "source": [
    "query = \"Where do I study and What do I study\"\n",
    "print(answer(query))\n",
    "print(f\"No of Tokens: {n_tokens(query)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86c268ea-6ecf-4e92-b6b9-ba4385b763e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You study software engineering, and you are at FAST-NUCES.\n",
      "No of Tokens: 15\n",
      "You are 19 years old and graduated from the AWS re/Start program.\n",
      "No of Tokens: 17\n",
      "You are 19 years old and graduated from the AWS re/Start program.\n",
      "No of Tokens: 11\n",
      "You have experience with cloud computing and servers.\n",
      "No of Tokens: 9\n",
      "You have experience with cloud computing and servers.\n",
      "No of Tokens: 7\n",
      "You have experience with Java and are currently working on a Transport Management System.\n",
      "No of Tokens: 12\n",
      "You are using Java for your transport system project.\n",
      "No of Tokens: 11\n",
      "You are building a PDF management system using AWS S3 and FastAPI.\n",
      "No of Tokens: 15\n",
      "You are using AWS S3 for file storage.\n",
      "No of Tokens: 9\n",
      "You have a powerful PC with an RTX 4090 GPU for AI model deployment.\n",
      "No of Tokens: 20\n",
      "You use an RTX 4090 GPU for running AI models locally.\n",
      "No of Tokens: 10\n",
      "You are designing a mentorship program for individuals interested in AI and machine learning, likely focusing on practical applications and local GPU resources.\n",
      "No of Tokens: 12\n",
      "You like creating faceless finance and tech videos using T4 GPU servers.\n",
      "No of Tokens: 15\n",
      "You study using T4 GPU servers, likely focusing on technology and finance related topics.\n",
      "No of Tokens: 9\n",
      "You are designing a 12-week AWS mentorship program, and it is for cloud technicians.\n",
      "No of Tokens: 16\n"
     ]
    }
   ],
   "source": [
    "query = \"My name is Ali, I study software engineering at FAST-NUCES.\"\n",
    "print(answer(query))\n",
    "print(f\"No of Tokens: {n_tokens(query)}\")\n",
    "\n",
    "query = \"I am 19 years old and I graduated from AWS re/Start program.\"\n",
    "print(answer(query))\n",
    "print(f\"No of Tokens: {n_tokens(query)}\")\n",
    "\n",
    "query = \"What is my age and which program did I graduate from\"\n",
    "print(answer(query))\n",
    "print(f\"No of Tokens: {n_tokens(query)}\")\n",
    "\n",
    "query = \"I have experience with cloud computing and servers.\"\n",
    "print(answer(query))\n",
    "print(f\"No of Tokens: {n_tokens(query)}\")\n",
    "\n",
    "query = \"What technologies do I have experience in\"\n",
    "print(answer(query))\n",
    "print(f\"No of Tokens: {n_tokens(query)}\")\n",
    "\n",
    "query = \"I am currently working on a Java-based Transport Management System.\"\n",
    "print(answer(query))\n",
    "print(f\"No of Tokens: {n_tokens(query)}\")\n",
    "\n",
    "query = \"Which programming language am I using for my transport system project\"\n",
    "print(answer(query))\n",
    "print(f\"No of Tokens: {n_tokens(query)}\")\n",
    "\n",
    "query = \"I am building a PDF management system using AWS S3 and FastAPI.\"\n",
    "print(answer(query))\n",
    "print(f\"No of Tokens: {n_tokens(query)}\")\n",
    "\n",
    "query = \"Which cloud service am I using for file storage\"\n",
    "print(answer(query))\n",
    "print(f\"No of Tokens: {n_tokens(query)}\")\n",
    "\n",
    "query = \"I have a powerful PC with an RTX 4090 GPU for AI model deployment.\"\n",
    "print(answer(query))\n",
    "print(f\"No of Tokens: {n_tokens(query)}\")\n",
    "\n",
    "query = \"Which GPU do I use for running AI models locally\"\n",
    "print(answer(query))\n",
    "print(f\"No of Tokens: {n_tokens(query)}\")\n",
    "\n",
    "query = \"What kind of mentorship program am I designing and for whom\"\n",
    "print(answer(query))\n",
    "print(f\"No of Tokens: {n_tokens(query)}\")\n",
    "\n",
    "query = \"I like creating faceless finance and tech videos using T4 GPU servers.\"\n",
    "print(answer(query))\n",
    "print(f\"No of Tokens: {n_tokens(query)}\")\n",
    "\n",
    "query = \"Where do I study and What do I study\"\n",
    "print(answer(query))\n",
    "print(f\"No of Tokens: {n_tokens(query)}\")\n",
    "\n",
    "query = \"I am designing a 12-week AWS mentorship program for cloud technicians.\"\n",
    "print(answer(query))\n",
    "print(f\"No of Tokens: {n_tokens(query)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca35f89-185c-4cc1-b28a-4ee96bfb154f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
