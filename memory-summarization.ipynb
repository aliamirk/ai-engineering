{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "724b87ed-2f8f-4b65-80c5-2a710c682c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_cpp\n",
    "\n",
    "class LLMWithAutoSummarization:\n",
    "    \"\"\"\n",
    "    Chat wrapper around llama-cpp that keeps a single running summary \n",
    "    of the whole conversation. The raw dialogue is **never** sent back \n",
    "    to the model once it has been summarized..\n",
    "    \"\"\"\n",
    "    \n",
    "    SYSTEM_PROMPT = (\n",
    "        \"You are a friendly and knowledgeable AI assistant. \"\n",
    "        \"IMPORTANT: You have access to a summary of your previous conversation with the user. \"\n",
    "        \"This summary contains facts you learned about the user (like their name, preferences, etc.) \"\n",
    "        \"and topics you discussed. When answering questions, USE THIS SUMMARY as if it is your memory. \"\n",
    "        \"If the user asks about something mentioned in the summary, answer confidently based on that information. \"\n",
    "        \"Speak in first person ('I') and address the user directly ('you').\"\n",
    "    )\n",
    "    \n",
    "    CONTEXT_WINDOW = 10240\n",
    "    SUMMARY_TOKEN_BUDGET = 1024\n",
    "    GENERATION_MAX_TOKENS = 2048\n",
    "    \n",
    "    def __init__(self, model_path: str = \"/home/ubuntu/ai-engineering/models/qwen2.5-7b-instruct-q5_k_m-00001-of-00002.gguf\"):\n",
    "        self.llm = llama_cpp.Llama(\n",
    "            model_path=model_path,\n",
    "            n_gpu_layers=-1,\n",
    "            n_ctx=self.CONTEXT_WINDOW,\n",
    "            verbose=False,\n",
    "        )\n",
    "        self.summary: str = \"\"\n",
    "        self.generation_params = {\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"stop\": [\"<|im_end|>\", \"<|endoftext|>\"],\n",
    "            \"max_tokens\": self.GENERATION_MAX_TOKENS,\n",
    "        }\n",
    "    \n",
    "    def answer(self, user_text: str) -> str:\n",
    "        \"\"\"Generate a reply and immediately update the running summary.\"\"\"\n",
    "        # --- Phase 1: Answer ---\n",
    "        prompt_msgs = [{\"role\": \"system\", \"content\": self.SYSTEM_PROMPT}]\n",
    "        \n",
    "        if self.summary:\n",
    "            prompt_msgs.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"HERE IS WHAT YOU KNOW FROM YOUR PREVIOUS CONVERSATION:\\n\\n\"\n",
    "                    f\"{self.summary}\\n\\n\"\n",
    "                    \"Use this information to answer the user's questions. \"\n",
    "                    \"If they ask about their name, location, preferences, or anything else \"\n",
    "                    \"mentioned above, provide that information confidently.\"\n",
    "                )\n",
    "            })\n",
    "        \n",
    "        prompt_msgs.append({\"role\": \"user\", \"content\": user_text})\n",
    "        \n",
    "        reply = self.llm.create_chat_completion(\n",
    "            messages=prompt_msgs,\n",
    "            **self.generation_params\n",
    "        )[\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        # --- Phase 2: Update memory ---\n",
    "        self._update_summary(user_text, reply)\n",
    "        \n",
    "        return reply\n",
    "    \n",
    "    def _update_summary(self, user_text: str, assistant_text: str) -> None:\n",
    "        \"\"\"\n",
    "        Roll the latest exchange into the running summary.\n",
    "        Uses the same model but lower temperature and token limit.\n",
    "        \"\"\"\n",
    "        SUMMARIZATION_SYSTEM_PROMPT = \"\"\"\n",
    "        You are creating a memory summary. Output ONLY the summary itself with no extra commentary. \n",
    "        Format:\n",
    "        USER PROFILE: \n",
    "        - Name: [if mentioned] \n",
    "        - Location: [if mentioned] \n",
    "        - Other details: [list any other personal info] \n",
    "        \n",
    "        CONVERSATION TOPICS: \n",
    "        - [topic 1] \n",
    "        - [topic 2] \n",
    "        \n",
    "        PREFERENCES/REQUESTS: \n",
    "        - [any stated preferences] \n",
    "        \n",
    "        Rules: \n",
    "        1. ALWAYS keep the user's name if it was ever mentioned \n",
    "        2. Keep it concise but never lose important facts \n",
    "        3. NO explanations or notes - output ONLY the formatted summary \n",
    "        4. If old topics become irrelevant, you can remove them \n",
    "        5. Consolidate similar information\n",
    "        \"\"\"\n",
    "        \n",
    "        text_to_summarize = f\"\"\"\n",
    "        Previous summary:\n",
    "        {self.summary or 'No previous conversation'}\n",
    "        \n",
    "        New exchange:\n",
    "        User: {user_text}\n",
    "        Assistant: {assistant_text}\n",
    "        \n",
    "        Output the updated summary using the format above. Remember: NO extra commentary, just the summary.\n",
    "        \"\"\"\n",
    "        \n",
    "        summarization_messages = [\n",
    "        {\"role\": \"system\", \"content\": SUMMARIZATION_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": text_to_summarize}\n",
    "        ]\n",
    "        \n",
    "        new_summary = llm.create_chat_completion(\n",
    "        messages=summarization_messages,\n",
    "        temperature=0.1, # Low temp for determined output\n",
    "        max_tokens=SUMMARY_TOKEN_BUDGET,\n",
    "        stop=[\"<|im_end|>\", \"<|endoftext|>\"],\n",
    "        )[\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        self.summary = new_summary.strip()\n",
    "    \n",
    "    def print_memory(self) -> None:\n",
    "        \"\"\"Inspect the current summary.\"\"\"\n",
    "        print(\"\\n=== Running Summary ===\")\n",
    "        print(self.summary or \"(empty)\")\n",
    "        print(\"=======================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f67b534-dc54-4d72-a621-d9dcc5ef91f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (10240) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Alice! It's nice to meet you again. How can I assist you today?\n",
      "--- Turn 2 ---\n",
      "Hello Alice! Did you know that the number 0 is considered an even number? It's unique because it's the only integer that is neither positive nor negative, but it plays a crucial role in the system of even and odd numbers. Isn't that interesting?\n",
      "--- The Moment of Truth ---\n",
      "Your name is Alice. It's great to chat with you again, Alice!\n",
      "--- Turn 3 ---\n",
      "Hello Alice! Did you know that the number 1 is considered neither prime nor composite? It's a unique number with its own special properties in mathematics.\n",
      "--- The Moment of Truth ---\n",
      "Your name is Alice. It was nice to greet you earlier, Alice!\n",
      "--- Turn 4 ---\n",
      "Hello Alice! Did you know that the number 2 is considered the first prime number and the only even prime number? Isn't that interesting?\n",
      "--- The Moment of Truth ---\n",
      "Your name is Alice. It was nice to greet you earlier, Alice!\n",
      "--- Turn 5 ---\n",
      "Hello Alice! Did you know that the number 3 is considered a lucky number in many cultures? It often symbolizes harmony and balance.\n",
      "--- The Moment of Truth ---\n",
      "Your name is Alice. Nice to see you again!\n",
      "--- Turn 6 ---\n",
      "Hello Alice! Did you know that the number 4 is considered unlucky in many East Asian cultures, including China, Japan, and Korea? This is because the pronunciation of the number 4 in Chinese (si) sounds similar to the word for \"death\" (si).\n",
      "--- The Moment of Truth ---\n",
      "Your name is Alice. Nice to see you again!\n",
      "--- Turn 7 ---\n",
      "Hello Alice! Did you know that the number 5 is considered a prime number, which means it is only divisible by 1 and itself? It's a fascinating property that makes 5 unique in the world of numbers!\n",
      "--- The Moment of Truth ---\n",
      "Your name is Alice. It's great to see you again, Alice!\n",
      "--- Turn 8 ---\n",
      "Hello Alice! Did you know that the number 6 is considered a perfect number? This means that the sum of its positive divisors (excluding itself) adds up to the number itself. For 6, its divisors are 1, 2, and 3, and when you add them together, you get 6! Isn't that fascinating?\n",
      "--- The Moment of Truth ---\n",
      "Your name is Alice. It was nice to greet you earlier, Alice!\n",
      "--- Turn 9 ---\n",
      "Hello Alice! Did you know that the number 7 is considered lucky in many cultures? For example, it's common to find 7 blessings or 7 heavens in various religious texts and traditions. Isn't that interesting?\n",
      "--- The Moment of Truth ---\n",
      "Your name is Alice. It's nice to see you again, Alice!\n",
      "--- Turn 10 ---\n",
      "Hello Alice! Did you know that the number 8 is considered a lucky number in Chinese culture? It's often associated with wealth and prosperity because it sounds similar to the word for \"prosper\" or \"fortune\" in Cantonese. Isn't that interesting?\n",
      "--- The Moment of Truth ---\n",
      "Your name is Alice. It's great to see you again, Alice!\n",
      "--- Turn 11 ---\n",
      "Hello Alice! Did you know that the number 9 is considered a lucky number in Chinese culture? It's associated with the word \"longevity\" because it sounds similar to the word for \"long life.\" Isn't that fascinating?\n",
      "--- The Moment of Truth ---\n",
      "Your name is Alice. It's great to see you again, Alice!\n"
     ]
    }
   ],
   "source": [
    "llm_with_memory = LLMWithAutoSummarization()\n",
    "print(llm_with_memory.answer(\"My name is Ali.\")) # Turn 1 \n",
    "\n",
    "# Simulate 10 more turns of conversation about something else \n",
    "for i in range(10):\n",
    " print(f\"--- Turn {i+2} ---\")\n",
    " print(llm_with_memory.answer(f\"Tell me a random fact about the number {i}.\"))\n",
    " # Now, ask the critical question again \n",
    " print(\"--- The Moment of Truth ---\")\n",
    "\n",
    "print(llm_with_memory.answer(\"What is my name?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7e64778-f7af-4acd-9211-d93f7307cc75",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LLMWithAutoSummarization' object has no attribute 'create_chat_completion'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43manswer\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMy name is Ali. I am a cloud engineer.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m) \n\u001b[32m      2\u001b[39m llm_with_memory.print_memory() \n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(llm.answer(\u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m'\u001b[39m\u001b[33mm working on a weapon detection project.\u001b[39m\u001b[33m\"\u001b[39m)) \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mLLMWithAutoSummarization.answer\u001b[39m\u001b[34m(self, user_text)\u001b[39m\n\u001b[32m     57\u001b[39m reply = \u001b[38;5;28mself\u001b[39m.llm.create_chat_completion(\n\u001b[32m     58\u001b[39m     messages=prompt_msgs,\n\u001b[32m     59\u001b[39m     **\u001b[38;5;28mself\u001b[39m.generation_params\n\u001b[32m     60\u001b[39m )[\u001b[33m\"\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# --- Phase 2: Update memory ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreply\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m reply\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 111\u001b[39m, in \u001b[36mLLMWithAutoSummarization._update_summary\u001b[39m\u001b[34m(self, user_text, assistant_text)\u001b[39m\n\u001b[32m     95\u001b[39m text_to_summarize = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[33mPrevious summary:\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[33m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.summary\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mNo previous conversation\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33mOutput the updated summary using the format above. Remember: NO extra commentary, just the summary.\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    106\u001b[39m summarization_messages = [\n\u001b[32m    107\u001b[39m {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: SUMMARIZATION_SYSTEM_PROMPT},\n\u001b[32m    108\u001b[39m {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: text_to_summarize}\n\u001b[32m    109\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m new_summary = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_chat_completion\u001b[49m(\n\u001b[32m    112\u001b[39m messages=summarization_messages,\n\u001b[32m    113\u001b[39m temperature=\u001b[32m0.1\u001b[39m, \u001b[38;5;66;03m# Low temp for consistent formatting\u001b[39;00m\n\u001b[32m    114\u001b[39m max_tokens=SUMMARY_TOKEN_BUDGET,\n\u001b[32m    115\u001b[39m stop=[\u001b[33m\"\u001b[39m\u001b[33m<|im_end|>\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m<|endoftext|>\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    116\u001b[39m )[\u001b[33m\"\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    118\u001b[39m \u001b[38;5;28mself\u001b[39m.summary = new_summary.strip()\n",
      "\u001b[31mAttributeError\u001b[39m: 'LLMWithAutoSummarization' object has no attribute 'create_chat_completion'"
     ]
    }
   ],
   "source": [
    "print(llm.answer(\"My name is Ali. I am a cloud engineer.\")) \n",
    "llm_with_memory.print_memory() \n",
    "print(llm.answer(\"I'm working on a weapon detection project.\")) \n",
    "llm_with_memory.print_memory() \n",
    "print(llm.answer(\"I prefer Python over R becuase python has much larger ecosystem and is relatively easier for me to understand.\"))\n",
    "llm_with_memory.print_memory() \n",
    "print(llm.answer(\"What programming language should I use for deep learning?\"))\n",
    "llm_with_memory.print_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5705077d-7f96-47d6-b508-a78e5546fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup = \"\"\"\n",
    "Here are some things to remember:\n",
    "- My deployment bucket is s3://proj-847-staging-west\n",
    "- The build flag is --env=qa-cluster-3  \n",
    "- My SSH alias is devbox-7b\n",
    "- The port I always use is 9473\n",
    "- My teammate's code review tag is @chen-review-squad\n",
    "\"\"\"\n",
    "\n",
    "print(llm.answer(setup))\n",
    "llm.print_memory()\n",
    "\n",
    "# Turn 2-6: Distractor chat\n",
    "# We need to push the first instruction out of the immediate context\n",
    "distractors = [\n",
    "    \"How do I center a div?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain the difference between TCP and UDP.\",\n",
    "    \"Write a haiku about coding.\",\n",
    "    \"What is 12 * 12?\"\n",
    "]\n",
    "\n",
    "for d in distractors:\n",
    "    print(f\"User: {d}\")\n",
    "    print(f\"Bot: {llm.answer(d)}\")\n",
    "\n",
    "# The Moment of Truth\n",
    "print(\"--- RETRIEVAL TEST ---\")\n",
    "print(llm.answer(\"What port do I always use?\"))\n",
    "llm.print_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
