{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Callable, Any\n",
    "\n",
    "\n",
    "class Tool:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        description: str,\n",
    "        input_schema: Dict[str, Any],\n",
    "        output_schema: Dict[str, Any],\n",
    "        func: Callable[..., Any],\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.input_schema = input_schema\n",
    "        self.output_schema = output_schema\n",
    "        self.func = func\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        return self.func(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class ToolRegistry:\n",
    "    def __init__(self):\n",
    "        self.tools: Dict[str, Tool] = {}\n",
    "\n",
    "    def register(self, tool: Tool):\n",
    "        self.tools[tool.name] = tool\n",
    "\n",
    "    def get(self, name: str) -> Tool:\n",
    "        if name not in self.tools.keys():\n",
    "            raise ValueError(f\"Tool '{name}' not found\")\n",
    "        return self.tools[name]\n",
    "\n",
    "    def list_tools(self) -> List[Dict[str, Any]]:\n",
    "        return [\n",
    "            {\n",
    "                \"name\": tool.name,\n",
    "                \"description\": tool.description,\n",
    "                \"input_schema\": tool.input_schema.model_json_schema(),\n",
    "            }\n",
    "            for tool in self.tools.values()\n",
    "        ]\n",
    "\n",
    "    def get_tool_call_args_type(self) -> Union[BaseModel]:\n",
    "        input_args_models = [tool.input_schema for tool in self.tools.values()]\n",
    "        tool_call_args = Union[tuple(input_args_models)]\n",
    "        return tool_call_args\n",
    "\n",
    "    def get_tool_names(self) -> Literal[None]:\n",
    "        return Literal[*self.tools.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example tool definitions\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool input schemas for validation\n",
    "\n",
    "class ToolAddArgs(BaseModel):\n",
    "    a: int\n",
    "    b: int\n",
    "\n",
    "\n",
    "class ToolMultiplyArgs(BaseModel):\n",
    "    a: int\n",
    "    b: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool registry, create a new tool and add it to this registry then the agent will be able to call it\n",
    "\n",
    "registry = ToolRegistry()\n",
    "\n",
    "registry.register(Tool(\n",
    "    name=\"add\",\n",
    "    description=\"Add two numbers\",\n",
    "    input_schema=ToolAddArgs,\n",
    "    output_schema={\"result\": \"int\"},\n",
    "    func=add,\n",
    "))\n",
    "\n",
    "registry.register(Tool(\n",
    "    name=\"multiply\",\n",
    "    description=\"Multiply two numbers\",\n",
    "    input_schema=ToolMultiplyArgs,\n",
    "    output_schema={\"result\": \"int\"},\n",
    "    func=multiply,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Tools: typing.Literal['add', 'multiply']\n",
      "Tool Arguments: typing.Union[__main__.ToolAddArgs, __main__.ToolMultiplyArgs]\n"
     ]
    }
   ],
   "source": [
    "# Get type-safe tool names and arguments\n",
    "ToolNameLiteral = registry.get_tool_names()\n",
    "ToolArgsUnion = registry.get_tool_call_args_type()\n",
    "\n",
    "print(f\"Available Tools: {ToolNameLiteral}\\nTool Arguments: {ToolArgsUnion}\")\n",
    "\n",
    "class ToolCall(BaseModel):\n",
    "    action: Literal[\"tool\"]\n",
    "    thought: str\n",
    "    tool_name: ToolNameLiteral\n",
    "    args: ToolArgsUnion\n",
    "\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    action: Literal[\"final\"]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "LLMResponse = Union[ToolCall, FinalAnswer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "class GeminiLLM:\n",
    "    def __init__(self, client, tool_registry, model=\"gemini-2.5-flash\"):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.tool_registry = tool_registry\n",
    "        self.system_instruction = self._create_system_instruction()\n",
    "        \n",
    "    def _create_system_instruction(self):\n",
    "        tools_info = self.tool_registry.list_tools()\n",
    "        print(tools_info)\n",
    "        \n",
    "        system_prompt = f\"\"\"\n",
    "        You are a conversational AI agent that can interact with external tools.\n",
    "        \n",
    "        CRITICAL RULES (MUST FOLLOW):\n",
    "        - You are NOT allowed to perform operations internally that could be performed by an available tool.\n",
    "        - If a tool exists that can perform any part of the task, you MUST use that tool.\n",
    "        - You MUST NOT skip tools, even for simple or obvious steps.\n",
    "        - You MUST NOT combine multiple operations into a single step unless a tool explicitly supports it.\n",
    "        - You may ONLY produce a final answer when no available tool can further advance the task.\n",
    "        TOOL USAGE RULES:\n",
    "        - Each tool call must perform exactly ONE meaningful operation.\n",
    "        - If the task requires multiple operations, you MUST call tools sequentially.\n",
    "        - If multiple tools could apply, choose the most specific one.\n",
    "        RESPONSE FORMAT (STRICT):\n",
    "        - You MUST respond ONLY in valid JSON.\n",
    "        - Never include explanations outside JSON.\n",
    "        - You must choose exactly one action per response.\n",
    "        \n",
    "        Available tools:\n",
    "        {json.dumps(tools_info, indent=2)}\n",
    "        \n",
    "        To use a tool, respond with a JSON object with the following structure:\n",
    "        {{\n",
    "            \"action\": \"tool\",\n",
    "            \"thought\": \"Your reasoning here\",\n",
    "            \"tool_name\": \"name of the tool to call\",\n",
    "            \"inputs\": \"tool inputs here\"\n",
    "        }}\n",
    "        \n",
    "        To give a final answer, respond with a JSON object with the following structure:\n",
    "        {{\n",
    "            \"action\": \"final\",\n",
    "            \"answer\": \"your final answer here\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "        return system_prompt\n",
    "    \n",
    "    \n",
    "    # converting the messages to gemini native request body\n",
    "    # each llm will need to have its own native messages formatter\n",
    "    \n",
    "    def _format_gemini_chat_history(self, history: list[dict]) -> list:\n",
    "        formatted_history = []\n",
    "        for message in history:\n",
    "            if message[\"role\"] == \"user\":\n",
    "                formatted_history.append(types.Content(\n",
    "                        role=\"user\",\n",
    "                        parts=[\n",
    "                            types.Part.from_text(text=message[\"content\"])\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            if message[\"role\"] == \"assistant\":\n",
    "                formatted_history.append(types.Content(\n",
    "                        role=\"model\",\n",
    "                        parts=[\n",
    "                            types.Part.from_text(text=message[\"content\"])\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            if message[\"role\"] == \"tool\":\n",
    "                formatted_history.append(types.Content(\n",
    "                        role=\"tool\",\n",
    "                        parts=[\n",
    "                            types.Part.from_function_response(\n",
    "                                name=message[\"tool_name\"],\n",
    "                                response={'result': message[\"tool_response\"]},\n",
    "                            )\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "        return formatted_history\n",
    "    \n",
    "    # finally a generate function with structured output based on the schema defined above\n",
    "    def generate(self, history: list[dict]) -> str:\n",
    "        gemini_history_format = self._format_gemini_chat_history(history)\n",
    "        response = self.client.models.generate_content(\n",
    "            model=self.model,\n",
    "            contents=gemini_history_format,\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=0,\n",
    "                response_mime_type=\"application/json\",\n",
    "                response_schema=LLMResponse,\n",
    "                system_instruction=self.system_instruction,\n",
    "                automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)\n",
    "            ),\n",
    "        )\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main agent orchestrator\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, llm, tool_registry, max_steps=5):\n",
    "        self.llm = llm\n",
    "        self.tool_registry = tool_registry\n",
    "        self.history = []\n",
    "        self.max_steps = max_steps # prevent infinite loops\n",
    "        \n",
    "    def run(self, user_input: str):\n",
    "        self.history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        for step in range(self.max_steps):\n",
    "            # Get LLM decision\n",
    "            llm_output = self.llm.generate(self.history)\n",
    "            action = json.loads(llm_output)\n",
    "            if action[\"action\"] == \"tool\":\n",
    "                # Record the thought process\n",
    "                self.history.append(\n",
    "                    {\"role\": \"assistant\", \"content\": llm_output}\n",
    "                )\n",
    "                # Execute the tool\n",
    "                tool = self.tool_registry.get(action[\"tool_name\"])\n",
    "                result = tool(**action[\"args\"])\n",
    "                # Record the result\n",
    "                observation = f\"Tool {tool.name} returned: {result}\"\n",
    "                self.history.append(\n",
    "                    {\"role\": \"tool\", \"tool_name\": tool.name, \"tool_response\": result}\n",
    "                )\n",
    "                continue\n",
    "            if action[\"action\"] == \"final\":\n",
    "                self.history.append(\n",
    "                    {\"role\": \"assistant\", \"content\": llm_output}\n",
    "                )\n",
    "                return action[\"answer\"]\n",
    "        raise RuntimeError(\"Agent did not terminate within max_steps\")\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'add', 'description': 'Add two numbers', 'input_schema': {'properties': {'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}, 'required': ['a', 'b'], 'title': 'ToolAddArgs', 'type': 'object'}}, {'name': 'multiply', 'description': 'Multiply two numbers', 'input_schema': {'properties': {'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}, 'required': ['a', 'b'], 'title': 'ToolMultiplyArgs', 'type': 'object'}}]\n",
      "Welcome! Type 'exit' to quit.\\n\n",
      "Agent: 16\n",
      "Agent: 25\n",
      "Goodbye!\n",
      "[{'content': 'What is 5 plus 3, then multiply the result by 2?',\n",
      "  'role': 'user'},\n",
      " {'content': '{\\n'\n",
      "             '  \"action\": \"tool\",\\n'\n",
      "             '  \"thought\": \"The user wants to calculate \\'5 plus 3\\' first. I '\n",
      "             'should use the \\'add\\' tool for this operation.\",\\n'\n",
      "             '  \"tool_name\": \"add\",\\n'\n",
      "             '  \"args\": {\\n'\n",
      "             '    \"a\": 5,\\n'\n",
      "             '    \"b\": 3\\n'\n",
      "             '  }\\n'\n",
      "             '}',\n",
      "  'role': 'assistant'},\n",
      " {'role': 'tool', 'tool_name': 'add', 'tool_response': 8},\n",
      " {'content': '{\\n'\n",
      "             '  \"action\": \"tool\",\\n'\n",
      "             '  \"thought\": \"The previous operation (5 + 3) resulted in 8. Now '\n",
      "             \"I need to multiply this result by 2, as per the user's \"\n",
      "             'request.\",\\n'\n",
      "             '  \"tool_name\": \"multiply\",\\n'\n",
      "             '  \"args\": {\\n'\n",
      "             '    \"a\": 8,\\n'\n",
      "             '    \"b\": 2\\n'\n",
      "             '  }\\n'\n",
      "             '}',\n",
      "  'role': 'assistant'},\n",
      " {'role': 'tool', 'tool_name': 'multiply', 'tool_response': 16},\n",
      " {'content': '{\\n  \"action\": \"final\",\\n  \"answer\": \"16\"\\n}',\n",
      "  'role': 'assistant'},\n",
      " {'content': 'what is 7*3+4', 'role': 'user'},\n",
      " {'content': '{\\n'\n",
      "             '  \"action\": \"tool\",\\n'\n",
      "             '  \"thought\": \"The user wants to calculate \\'7*3+4\\'. Following '\n",
      "             'the order of operations, I must perform the multiplication '\n",
      "             'first. I will use the \\'multiply\\' tool to calculate 7 * 3.\",\\n'\n",
      "             '  \"tool_name\": \"multiply\",\\n'\n",
      "             '  \"args\": {\\n'\n",
      "             '    \"a\": 7,\\n'\n",
      "             '    \"b\": 3\\n'\n",
      "             '  }\\n'\n",
      "             '}',\n",
      "  'role': 'assistant'},\n",
      " {'role': 'tool', 'tool_name': 'multiply', 'tool_response': 21},\n",
      " {'content': '{\\n'\n",
      "             '  \"action\": \"tool\",\\n'\n",
      "             '  \"thought\": \"The previous step calculated 7 * 3, which resulted '\n",
      "             'in 21. Now I need to add 4 to this result to complete the '\n",
      "             'calculation \\'7*3+4\\'. I will use the \\'add\\' tool for this.\",\\n'\n",
      "             '  \"tool_name\": \"add\",\\n'\n",
      "             '  \"args\": {\\n'\n",
      "             '    \"a\": 21,\\n'\n",
      "             '    \"b\": 4\\n'\n",
      "             '  }\\n'\n",
      "             '}',\n",
      "  'role': 'assistant'},\n",
      " {'role': 'tool', 'tool_name': 'add', 'tool_response': 25},\n",
      " {'content': '{\\n  \"action\": \"final\",\\n  \"answer\": \"25\"\\n}',\n",
      "  'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    raise RuntimeError(\"GEMINI_API_KEY environment variable not set\")\n",
    "\n",
    "# Initialize the client\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "# Create LLM and Agent\n",
    "llm = GeminiLLM(client, registry)\n",
    "agent = Agent(llm, registry)\n",
    "\n",
    "\n",
    "def chat_with_agent(agent: Agent):\n",
    "    print(\"Welcome! Type 'exit' to quit.\\\\n\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            pprint(agent.history)\n",
    "            break\n",
    "        try:\n",
    "            response = agent.run(user_input)\n",
    "            print(f\"Agent: {response}\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Agent error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "\n",
    "\n",
    "# Start chatting\n",
    "chat_with_agent(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/apple/Desktop/llms stuff/universal_agent/.venv/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gemini-venv)",
   "language": "python",
   "name": "gemini-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
