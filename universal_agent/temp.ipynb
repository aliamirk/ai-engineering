{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "BASE_URL = os.getenv(\"BASE_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Callable, Optional, Any\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class Tool:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        description: str,\n",
    "        input_schema: Dict[str, Any],\n",
    "        output_schema: Dict[str, Any],\n",
    "        func: Callable[..., Any],\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.input_schema = input_schema\n",
    "        self.output_schema = output_schema\n",
    "        self.func = func\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        return self.func(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class ToolRegistry:\n",
    "    def __init__(self):\n",
    "        self.tools: Dict[str, Tool] = {}\n",
    "\n",
    "    def register(self, tool: Tool):\n",
    "        self.tools[tool.name] = tool\n",
    "\n",
    "    def get(self, name: str) -> Tool:\n",
    "        if name not in self.tools.keys():\n",
    "            raise ValueError(f\"Tool '{name}' not found\")\n",
    "        return self.tools[name]\n",
    "\n",
    "    def list_tools(self) -> List[Dict[str, Any]]:\n",
    "        return [\n",
    "            {\n",
    "                \"name\": tool.name,\n",
    "                \"description\": tool.description,\n",
    "                \"input_schema\": tool.input_schema.model_json_schema(),\n",
    "            }\n",
    "            for tool in self.tools.values()\n",
    "        ]\n",
    "\n",
    "    def get_tool_call_args_type(self) -> Union[BaseModel]:\n",
    "        input_args_models = [tool.input_schema for tool in self.tools.values()]\n",
    "        tool_call_args = Union[tuple(input_args_models)]\n",
    "        return tool_call_args\n",
    "\n",
    "    def get_tool_names(self) -> Literal[None]:\n",
    "        return Literal[*self.tools.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool definitions\n",
    "\n",
    "import requests\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    # API call here\n",
    "    return f\"Weather in {city}: Sunny, 72Â°F\"\n",
    "\n",
    "\n",
    "def list_all_gatepasses(status: Optional[str] = None) -> List[Dict[str, Any]]:\n",
    "    url = f\"{BASE_URL}/hr/gatepass/list\"\n",
    "\n",
    "    params = {\"status\": status} if status else {}\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()  # raises exception for 4xx/5xx\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool input and output schemas for validation\n",
    "\n",
    "class ToolAddArgs(BaseModel):\n",
    "    a: int\n",
    "    b: int\n",
    "\n",
    "\n",
    "class ToolMultiplyArgs(BaseModel):\n",
    "    a: int\n",
    "    b: int\n",
    "\n",
    "class WeatherArgs(BaseModel):\n",
    "    city: str\n",
    "    \n",
    "    \n",
    "class GatePassListArgs(BaseModel):\n",
    "    status: Optional[str] = None\n",
    "    \n",
    "class GatePassOut(BaseModel):\n",
    "    id: str\n",
    "    number: str\n",
    "    person_name: str\n",
    "    description: str\n",
    "    created_by: str\n",
    "    is_returnable: bool\n",
    "    status: str\n",
    "    status_history: List[str]\n",
    "    created_at: datetime\n",
    "    approved_at: Optional[datetime]\n",
    "    exit_photo_id: Optional[str]\n",
    "    return_photo_id: Optional[str]\n",
    "    exit_time: Optional[datetime]\n",
    "    return_time: Optional[datetime]\n",
    "    qr_code_url: Optional[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool registry, create a new tool and add it to this registry then the agent will be able to call it\n",
    "\n",
    "registry = ToolRegistry()\n",
    "\n",
    "registry.register(Tool(\n",
    "    name=\"add\",\n",
    "    description=\"Add two numbers\",\n",
    "    input_schema=ToolAddArgs,\n",
    "    output_schema={\"result\": \"int\"},\n",
    "    func=add,\n",
    "))\n",
    "\n",
    "registry.register(Tool(\n",
    "    name=\"multiply\",\n",
    "    description=\"Multiply two numbers\",\n",
    "    input_schema=ToolMultiplyArgs,\n",
    "    output_schema={\"result\": \"int\"},\n",
    "    func=multiply,\n",
    "))\n",
    "\n",
    "registry.register(Tool(\n",
    "    name=\"get_weather\",\n",
    "    description=\"Get current weather for a city\",\n",
    "    input_schema=WeatherArgs,\n",
    "    output_schema={\"Weather\": \"str\"},\n",
    "    func=get_weather,\n",
    "))\n",
    "\n",
    "registry.register(\n",
    "    Tool(\n",
    "        name=\"list_gatepasses\",\n",
    "        description=\"List all gatepasses, optionally filtered by status\",\n",
    "        input_schema=GatePassListArgs,\n",
    "        output_schema={\"gatepasses\": \"List[GatePassOut]\"},\n",
    "        func=list_all_gatepasses,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Tools: typing.Literal['add', 'multiply', 'get_weather', 'list_gatepasses']\n",
      "Tool Arguments: typing.Union[__main__.ToolAddArgs, __main__.ToolMultiplyArgs, __main__.WeatherArgs, __main__.GatePassListArgs]\n"
     ]
    }
   ],
   "source": [
    "# Get type-safe tool names and arguments\n",
    "ToolNameLiteral = registry.get_tool_names()\n",
    "ToolArgsUnion = registry.get_tool_call_args_type()\n",
    "\n",
    "print(f\"Available Tools: {ToolNameLiteral}\\nTool Arguments: {ToolArgsUnion}\")\n",
    "\n",
    "class ToolCall(BaseModel):\n",
    "    action: Literal[\"tool\"]\n",
    "    thought: str\n",
    "    tool_name: ToolNameLiteral\n",
    "    args: ToolArgsUnion\n",
    "\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    action: Literal[\"final\"]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "LLMResponse = Union[ToolCall, FinalAnswer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "class GeminiLLM:\n",
    "    def __init__(self, client, tool_registry, model=\"gemini-2.5-flash\"):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.tool_registry = tool_registry\n",
    "        self.system_instruction = self._create_system_instruction()\n",
    "        \n",
    "    def _create_system_instruction(self):\n",
    "        tools_info = self.tool_registry.list_tools()\n",
    "        print(tools_info)\n",
    "        \n",
    "        system_prompt = f\"\"\"\n",
    "        You are a conversational AI agent that can interact with external tools.\n",
    "        \n",
    "        CRITICAL RULES (MUST FOLLOW):\n",
    "        - You are NOT allowed to perform operations internally that could be performed by an available tool.\n",
    "        - If a tool exists that can perform any part of the task, you MUST use that tool.\n",
    "        - You MUST NOT skip tools, even for simple or obvious steps.\n",
    "        - You MUST NOT combine multiple operations into a single step unless a tool explicitly supports it.\n",
    "        - You may ONLY produce a final answer when no available tool can further advance the task.\n",
    "        TOOL USAGE RULES:\n",
    "        - Each tool call must perform exactly ONE meaningful operation.\n",
    "        - If the task requires multiple operations, you MUST call tools sequentially.\n",
    "        - If multiple tools could apply, choose the most specific one.\n",
    "        RESPONSE FORMAT (STRICT):\n",
    "        - You MUST respond ONLY in valid JSON.\n",
    "        - Never include explanations outside JSON.\n",
    "        - You must choose exactly one action per response.\n",
    "        \n",
    "        Available tools:\n",
    "        {json.dumps(tools_info, indent=2)}\n",
    "        \n",
    "        To use a tool, respond with a JSON object with the following structure:\n",
    "        {{\n",
    "            \"action\": \"tool\",\n",
    "            \"thought\": \"Your reasoning here\",\n",
    "            \"tool_name\": \"name of the tool to call\",\n",
    "            \"inputs\": \"tool inputs here\"\n",
    "        }}\n",
    "        \n",
    "        To give a final answer, respond with a JSON object with the following structure:\n",
    "        {{\n",
    "            \"action\": \"final\",\n",
    "            \"answer\": \"your final answer here\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "        return system_prompt\n",
    "    \n",
    "    \n",
    "    # converting the messages to gemini native request body\n",
    "    # each llm will need to have its own native messages formatter\n",
    "    \n",
    "    def _format_gemini_chat_history(self, history: list[dict]) -> list:\n",
    "        formatted_history = []\n",
    "        for message in history:\n",
    "            if message[\"role\"] == \"user\":\n",
    "                formatted_history.append(types.Content(\n",
    "                        role=\"user\",\n",
    "                        parts=[\n",
    "                            types.Part.from_text(text=message[\"content\"])\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            if message[\"role\"] == \"assistant\":\n",
    "                formatted_history.append(types.Content(\n",
    "                        role=\"model\",\n",
    "                        parts=[\n",
    "                            types.Part.from_text(text=message[\"content\"])\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "            if message[\"role\"] == \"tool\":\n",
    "                formatted_history.append(types.Content(\n",
    "                        role=\"tool\",\n",
    "                        parts=[\n",
    "                            types.Part.from_function_response(\n",
    "                                name=message[\"tool_name\"],\n",
    "                                response={'result': message[\"tool_response\"]},\n",
    "                            )\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "        return formatted_history\n",
    "    \n",
    "    # finally a generate function with structured output based on the schema defined above\n",
    "    def generate(self, history: list[dict]) -> str:\n",
    "        gemini_history_format = self._format_gemini_chat_history(history)\n",
    "        response = self.client.models.generate_content(\n",
    "            model=self.model,\n",
    "            contents=gemini_history_format,\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=0,\n",
    "                response_mime_type=\"application/json\",\n",
    "                response_schema=LLMResponse,\n",
    "                system_instruction=self.system_instruction,\n",
    "                automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)\n",
    "            ),\n",
    "        )\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main agent orchestrator\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, llm, tool_registry, max_steps=5):\n",
    "        self.llm = llm\n",
    "        self.tool_registry = tool_registry\n",
    "        self.history = []\n",
    "        self.max_steps = max_steps # prevent infinite loops\n",
    "        \n",
    "    def run(self, user_input: str):\n",
    "        self.history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        for step in range(self.max_steps):\n",
    "            # Get LLM decision\n",
    "            llm_output = self.llm.generate(self.history)\n",
    "            action = json.loads(llm_output)\n",
    "            if action[\"action\"] == \"tool\":\n",
    "                # Record the thought process\n",
    "                self.history.append(\n",
    "                    {\"role\": \"assistant\", \"content\": llm_output}\n",
    "                )\n",
    "                # Execute the tool\n",
    "                tool = self.tool_registry.get(action[\"tool_name\"])\n",
    "                result = tool(**action[\"args\"])\n",
    "                # Record the result\n",
    "                observation = f\"Tool {tool.name} returned: {result}\"\n",
    "                print(observation)\n",
    "                self.history.append(\n",
    "                    {\"role\": \"tool\", \"tool_name\": tool.name, \"tool_response\": result}\n",
    "                )\n",
    "                continue\n",
    "            if action[\"action\"] == \"final\":\n",
    "                self.history.append(\n",
    "                    {\"role\": \"assistant\", \"content\": llm_output}\n",
    "                )\n",
    "                return action[\"answer\"]\n",
    "        raise RuntimeError(\"Agent did not terminate within max_steps\")\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'add', 'description': 'Add two numbers', 'input_schema': {'properties': {'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}, 'required': ['a', 'b'], 'title': 'ToolAddArgs', 'type': 'object'}}, {'name': 'multiply', 'description': 'Multiply two numbers', 'input_schema': {'properties': {'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}, 'required': ['a', 'b'], 'title': 'ToolMultiplyArgs', 'type': 'object'}}, {'name': 'get_weather', 'description': 'Get current weather for a city', 'input_schema': {'properties': {'city': {'title': 'City', 'type': 'string'}}, 'required': ['city'], 'title': 'WeatherArgs', 'type': 'object'}}, {'name': 'list_gatepasses', 'description': 'List all gatepasses, optionally filtered by status', 'input_schema': {'properties': {'status': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Status'}}, 'title': 'GatePassListArgs', 'type': 'object'}}]\n",
      "Welcome! Type 'exit' to quit.\\n\n",
      "Agent: Hello! How can I help you today?[:100]\n",
      "Goodbye!\n",
      "[{'content': 'hi', 'role': 'user'},\n",
      " {'content': '{\\n'\n",
      "             '  \"action\": \"final\",\\n'\n",
      "             '  \"answer\": \"Hello! How can I help you today?\"\\n'\n",
      "             '}',\n",
      "  'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    raise RuntimeError(\"GEMINI_API_KEY environment variable not set\")\n",
    "\n",
    "# Initialize the client\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "# Create LLM and Agent\n",
    "llm = GeminiLLM(client, registry)\n",
    "agent = Agent(llm, registry)\n",
    "\n",
    "\n",
    "def chat_with_agent(agent: Agent):\n",
    "    print(\"Welcome! Type 'exit' to quit.\\\\n\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            pprint(agent.history)\n",
    "            break\n",
    "        try:\n",
    "            response = agent.run(user_input)\n",
    "            print(f\"Agent: {response}[:100]\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Agent error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "\n",
    "\n",
    "# Start chatting\n",
    "chat_with_agent(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced version of previous agent - added human in the loop, memory, obersvability\n",
    "\n",
    "class MemoryStore:\n",
    "    def __init__(self, file_path: str, max_entries: int = 50):\n",
    "        self.file_path = file_path\n",
    "        self.max_entries = max_entries\n",
    "        self._ensure_file()\n",
    "\n",
    "    def _ensure_file(self):\n",
    "        if not os.path.exists(self.file_path):\n",
    "            with open(self.file_path, \"w\") as f:\n",
    "                json.dump([], f)\n",
    "\n",
    "    def load_all(self) -> List[dict]:\n",
    "        try:\n",
    "            with open(self.file_path, \"r\") as f:\n",
    "                return json.load(f)\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "    def append(self, entry: dict):\n",
    "        data = self.load_all()\n",
    "        data.append(entry)\n",
    "\n",
    "        with open(self.file_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "    def get_recent(self, limit: Optional[int] = None) -> list[dict]:\n",
    "        data = self.load_all()\n",
    "        limit = limit or self.max_entries\n",
    "        return data[-limit:]\n",
    "\n",
    "    def delete_all(self):\n",
    "        with open(self.file_path, \"w\") as f:\n",
    "            json.dump([], f)\n",
    "            \n",
    "# creating memory object\n",
    "memory_store = MemoryStore(\n",
    "        file_path=\"/Users/apple/Desktop/llms stuff/universal_agent/content/agent_memory.json\",\n",
    "        max_entries=10,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gemini-venv)",
   "language": "python",
   "name": "gemini-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
