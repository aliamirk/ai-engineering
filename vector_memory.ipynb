{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9972f2b5-ac50-434f-843b-c0ae6c206788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, collection_name=\"chat_memory\"):\n",
    "        self.client = chromadb.Client()\n",
    "        self.collection = self.client.create_collection(name=collection_name, get_or_create=True)\n",
    "        # Load a small, fast embedding model (approx 80MB)\n",
    "        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    def search(self, query: str, k=10):\n",
    "        # 1. Convert query to vector\n",
    "        query_vec = self.embedder.encode(query).tolist()\n",
    "        # 2. Search ChromaDB\n",
    "        results = self.collection.query(query_embeddings=[query_vec], n_results=k)\n",
    "        # 3. Unpack ugly Chroma format into a clean list\n",
    "        return [\n",
    "            {'text': results['documents'][0][i], 'id': results['ids'][0][i]} \n",
    "            for i in range(len(results['documents'][0]))\n",
    "        ]\n",
    "\n",
    "    def add(self, text: str, metadata: dict | None = None):\n",
    "        if metadata is None:\n",
    "            metadata = {}\n",
    "\n",
    "        metadata.setdefault(\"created_at\", time.time())\n",
    "\n",
    "        vector = self.embedder.encode(text).tolist()\n",
    "\n",
    "        self.collection.add(\n",
    "            ids=[str(uuid.uuid4())],\n",
    "            documents=[text],\n",
    "            embeddings=[vector],\n",
    "            metadatas=[metadata]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3527ef8-d942-40ef-823c-d823a87a70bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "class KeywordIndex:\n",
    "    def __init__(self):\n",
    "        self.corpus = []\n",
    "        self.bm25 = None\n",
    "\n",
    "    def add(self, text: str):\n",
    "        self.corpus.append(text)\n",
    "        # Note: In production, use a real index (Lucene/Elastic). \n",
    "        # Rebuilding BM25 every turn is fine for prototypes but O(N) slow.\n",
    "        tokenized_corpus = [doc.lower().split() for doc in self.corpus]\n",
    "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    def search(self, query: str, k=10):\n",
    "        if not self.bm25: return []\n",
    "        tokenized_query = query.lower().split()\n",
    "        return self.bm25.get_top_n(tokenized_query, self.corpus, n=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1699009c-7ba9-40a3-8964-40a3c4507070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "class Reranker:\n",
    "    def __init__(self):\n",
    "        # A model specifically trained to score (Query, Document) pairs\n",
    "        self.model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "    def rank(self, query: str, docs: list, top_k=3):\n",
    "        if not docs: return []\n",
    "        # Score every pair\n",
    "        pairs = [[query, doc] for doc in docs]\n",
    "        scores = self.model.predict(pairs)\n",
    "        # Sort by score descending\n",
    "        sorted_docs = sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)\n",
    "        return [doc for score, doc in sorted_docs][:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ad15e86-7357-4499-8bce-857c6cd52447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import llama_cpp\n",
    "from collections import deque\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "class LLMWithVectorMemory:\n",
    "    \"\"\"\n",
    "    Chat wrapper that uses Hybrid RAG (Vector + Keyword + Reranking)\n",
    "    to retrieve long-term context, while keeping a tiny short-term\n",
    "    buffer for immediate flow.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Static Configuration ---\n",
    "    CONTEXT_WINDOW        = 4096\n",
    "    MAX_GENERATION_TOKENS = 1024\n",
    "    SAFETY_MARGIN         = 32\n",
    "\n",
    "    # We keep a tiny short-term buffer so the model doesn't forget\n",
    "    # what was said literally 1 second ago (RAG is for long-term)\n",
    "    SHORT_TERM_K = 2\n",
    "\n",
    "    # How many chunks to fetch from each index before fusion\n",
    "    RETRIEVAL_CANDIDATES = 10\n",
    "    # How many chunks to actually show the LLM after reranking\n",
    "    FINAL_TOP_K = 3\n",
    "\n",
    "    SYSTEM_PROMPT = (\n",
    "        \"You are an AI assistant with access to a long-term memory. \"\n",
    "        \"A section labeled 'RELEVANT CONTEXT FROM MEMORY' may be provided below.\\n\"\n",
    "\n",
    "        \"INSTRUCTIONS:\\n\"\n",
    "        \"1. Use the context to answer the user's question accurately.\\n\"\n",
    "        \"2. If the user in the context provides a fact (e.g., 'My name is X', 'My job is Y'), \"\n",
    "        \"TRUST THE USER, even if the assistant in the context previously denied knowing it.\\n\"\n",
    "        \"3. If the answer is not in the context, rely on your general knowledge, or take it as a new information if it's a fact presented by the user.\\n\"\n",
    "        \"4. User might provide you new information or update existing one.\"\n",
    "        \"5. DO NOT mention the 'RELEVANT CONTEXT FROM MEMORY' words explicitly.\"\n",
    "        \"6. Be concise.\"\n",
    "    )\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_path: str = \"/home/ubuntu/ai-engineering/models/qwen2.5-7b-instruct-q5_k_m-00001-of-00002.gguf\"\n",
    "    ):\n",
    "        # --- 1. Llama Initialization ---\n",
    "        self.llm = llama_cpp.Llama(\n",
    "            model_path=model_path,\n",
    "            n_gpu_layers=-1,\n",
    "            n_ctx=self.CONTEXT_WINDOW,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # --- 2. Memory Components Initialization ---\n",
    "        print(\"Initializing Memory Components... (This may take a moment)\")\n",
    "        self.vector_store = VectorStore(collection_name=\"chat_memory\")\n",
    "        self.keyword_index = KeywordIndex()\n",
    "        self.reranker = Reranker()\n",
    "\n",
    "        # --- 3. State ---\n",
    "        # Short-term buffer (Deque)\n",
    "        self.short_term_history: deque = deque(maxlen=self.SHORT_TERM_K * 2)\n",
    "\n",
    "        self.generation_params = {\n",
    "            \"temperature\": 0.6, # Lower temp for RAG to reduce hallucination\n",
    "            \"top_p\": 0.9,\n",
    "            \"stop\": [\"<|im_end|>\", \"<|endoftext|>\"],\n",
    "            \"max_tokens\": self.MAX_GENERATION_TOKENS,\n",
    "        }\n",
    "\n",
    "    # --- Public API ---\n",
    "\n",
    "    def answer(self, user_text: str) -> str:\n",
    "        \"\"\"\n",
    "        1. Retrieve relevant context (Hybrid + Rerank).\n",
    "        2. Build Prompt (System + Context + Short History + User)\n",
    "        3. Generate.\n",
    "        4. Save to Memory.\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Retrieve Context\n",
    "        # We retrieve based on the user's CURRENT input\n",
    "        retrieved_context = self._retrieve_context(user_text)\n",
    "\n",
    "        # 2. Build Prompt\n",
    "        messages = self._build_rag_prompt(user_text, retrieved_context)\n",
    "\n",
    "        # 3. Call Model\n",
    "        reply = self.llm.create_chat_completion(\n",
    "            messages=messages,\n",
    "            **self.generation_params\n",
    "        )[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        # 4. Update State (Short-term & Long-term)\n",
    "        self._update_memory(user_text, reply)\n",
    "\n",
    "        return reply\n",
    "    \n",
    "    def print_debug(self, last_context: List[str]):\n",
    "        \"\"\"Helper to see what the RAG actually found.\"\"\"\n",
    "        print(\"\\n--- RAG DEBUG: What the model saw ---\")\n",
    "        if not last_context:\n",
    "            print(\"(No relevant memories found)\")\n",
    "        else:\n",
    "            for i, ctx in enumerate(last_context):\n",
    "                print(f\"[{i+1}] {ctx[:100]}...\") # Print first 100 chars\n",
    "        print(\"---------------------------------------\\n\")\n",
    "    \n",
    "    # --- Internal Orchestration ---\n",
    "\n",
    "    def _retrieve_context(self, query: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Executes the Hybrid Search + Reranking Pipeline.\n",
    "        \"\"\"\n",
    "        # A. Parallel Retrieval\n",
    "        # Get dense results (Vector)\n",
    "        # Note: VectorStore.search returns dicts with 'text', 'id', etc.\n",
    "        vector_results = self.vector_store.search(query, k=self.RETRIEVAL_CANDIDATES)\n",
    "        vector_texts = [r['text'] for r in vector_results]\n",
    "\n",
    "        # Get sparse results (Keyword)\n",
    "        # Note: KeywordIndex.search returns plain strings\n",
    "        keyword_texts = self.keyword_index.search(query, k=self.RETRIEVAL_CANDIDATES)\n",
    "\n",
    "        # B. Fusion (Simple Set Deduplication)\n",
    "        all_candidates = list(set(vector_texts + keyword_texts))\n",
    "\n",
    "        if not all_candidates:\n",
    "            return []\n",
    "        \n",
    "        # C. Reranking\n",
    "        # The heavy lifting: re-sort candidates by relevance to the specific query\n",
    "        top_docs = self.reranker.rank(query, all_candidates, top_k=self.FINAL_TOP_K)\n",
    "\n",
    "        return top_docs\n",
    "    \n",
    "    def _build_rag_prompt(self, user_text: str, context_chunks: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Constructs the chat format:\n",
    "        System (with Context) -> Short Term History -> Current User\n",
    "        \"\"\"\n",
    "\n",
    "        # Format context into a string\n",
    "        context_str = \"\"\n",
    "        if context_chunks:\n",
    "            context_str = \"\\nRELEVANT CONTEXT FROM MEMORY:\\n\"\n",
    "            for chunk in context_chunks:\n",
    "                context_str += f\"- {chunk}\\n\"\n",
    "        \n",
    "        # Inject context into System Prompt\n",
    "        # This is 'In-Context Learning'\n",
    "        system_content = f\"{self.SYSTEM_PROMPT}\\n{context_str}\"\n",
    "\n",
    "        messages = [{\"role\": \"system\", \"content\": system_content}]\n",
    "\n",
    "        # Add Short-Term History (Sliding Window)\n",
    "        # This ensures the model can handle \"What about the second one?\" references\n",
    "        messages.extend(list(self.short_term_history))\n",
    "\n",
    "        # Add Current User\n",
    "        messages.append({\"role\": \"user\", \"content\": user_text})\n",
    "\n",
    "        return messages\n",
    "\n",
    "    def _update_memory(self, user_text: str, assistant_reply: str):\n",
    "        \"\"\"\n",
    "        Saves the interaction to:\n",
    "        1. RAM (Short-term sliding window)\n",
    "        2. ChromaDB (Vector)\n",
    "        3. BM25 (Keyword)\n",
    "        \"\"\"\n",
    "        # Update Short Term\n",
    "        self.short_term_history.append({\"role\": \"user\", \"content\": user_text})\n",
    "        self.short_term_history.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "\n",
    "        # Save to Long Term Memory\n",
    "        # Strategy: Save the PAIR. Saving just user text loses context.\n",
    "        # Saving just assistant text loses the prompt.\n",
    "        memory_text = f\"User: {user_text}\\nAssistant: {assistant_reply}\"\n",
    "\n",
    "        # Metadata for filtering later (e.g., by time)\n",
    "        metadata = {\"timestamp\": time.time(), \"type\": \"conversation_turn\"}\n",
    "\n",
    "        # 1. Vector Write\n",
    "        self.vector_store.add(memory_text, metadata)\n",
    "\n",
    "        # 2. Keyword Write (Triggering the expensive rebuild)\n",
    "        self.keyword_index.add(memory_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bec137d3-de92-46cc-847f-c29060aead00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Memory Components... (This may take a moment)\n",
      "Sure, here's a summary of the information you provided:\n",
      "\n",
      "- Deployment bucket: s3://proj-847-staging-west\n",
      "- Build flag: --env=qa-cluster-3\n",
      "- SSH alias: devbox-7b\n",
      "- Port: 9473\n",
      "- Teammate's code review tag: @chen-review-squad\n",
      "\n",
      "Is there anything specific you need help with regarding this information?\n"
     ]
    }
   ],
   "source": [
    "bot = LLMWithVectorMemory()\n",
    "\n",
    "setup = \"\"\"\n",
    "Here are some things to remember:\n",
    "- My deployment bucket is s3://proj-847-staging-west\n",
    "- The build flag is --env=qa-cluster-3  \n",
    "- My SSH alias is devbox-7b\n",
    "- The port I always use is 9473\n",
    "- My teammate's code review tag is @chen-review-squad\n",
    "\"\"\"\n",
    "print(bot.answer(setup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e0f152-95a4-4a6c-a962-89d0a38e5c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: How do I center a div?\n"
     ]
    }
   ],
   "source": [
    "distractors = [\n",
    "    \"How do I center a div?\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain the difference between TCP and UDP.\",\n",
    "    \"Write a haiku about coding.\",\n",
    "    \"What is 12 * 12?\"\n",
    "]\n",
    "for d in distractors:\n",
    "    print(f\"User: {d}\")\n",
    "    print(f\"Bot: {bot.answer(d)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dc26d5-4a24-40f0-b5f8-e5c39230d39c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
